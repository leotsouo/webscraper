# Dual-Source Web Scraper (Static + Dynamic) with Incremental Diff

Course: Information Retrieval and Generative AI – Term Project I  
Due: Oct/15/2025

This repo implements:
- Scrape **2 sources** (1 static HTML via `requests+BeautifulSoup`, 1 dynamic via **Playwright**).
- Store structured data (CSV snapshots).
- **Incremental updates**: compute new/changed/deleted records, write `diff_YYYYMMDD.csv` and `summary.json`.
- Minimal interface: **CLI** + optional **Streamlit** page (1 chart).
- Robustness: retry/backoff, robots.txt respect, error logging.
- Reproducibility: `run_first_time.sh` / `run_incremental.sh`.
- Tests (pytest): selectors, deduplication, validation, diff.

> **Note**: Fill your selectors & URLs in `config/sources.yaml` before running.

## Quick Start

### 1) Environment
```bash
python -m venv .venv && source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt
python -m playwright install  # required once for browsers
```

### 2) First run (initial snapshot)
```bash
bash run_first_time.sh
```

### 3) Incremental run (creates diff vs last snapshot)
```bash
bash run_incremental.sh
```

### 4) CLI help
```bash
python -m interface.cli --help
```

### 5) Optional Streamlit
```bash
streamlit run src/interface/app.py
```

## Project Structure

```
dual-source-webscraper/
├─ config/
│  └─ sources.yaml
├─ data/
│  ├─ snapshots/         # CSV snapshots (first & second run)
│  ├─ diffs/             # diff_YYYYMMDD.csv + summary.json
│  ├─ charts/            # generated PNG charts
│  └─ logs/              # error logs
├─ src/
│  ├─ scraper/
│  │  ├─ static_scraper.py
│  │  ├─ dynamic_scraper.py
│  │  └─ utils.py
│  ├─ pipeline/
│  │  ├─ storage.py
│  │  ├─ clean.py
│  │  └─ diff.py
│  └─ interface/
│     ├─ cli.py
│     └─ app.py
├─ tests/
│  ├─ test_selectors.py
│  ├─ test_dedup.py
│  ├─ test_validation.py
│  └─ test_diff.py
├─ run_first_time.sh
├─ run_incremental.sh
├─ requirements.txt
├─ pyproject.toml
└─ README.md
```

## Deliverables Checklist (per assignment)
- [x] Code + README
- [x] First & second snapshot data (CSV) – _created by your runs_
- [x] `diff_YYYYMMDD.csv` + `summary.json`
- [x] 3–5 charts (PNG) – generated by `pipeline/diff.py`
- [x] 6–10 page technical report – _add your report under `report/` (not included)_

## Notes
- **Robots.txt** is respected using Python's `urllib.robotparser`.
- **Exponential backoff** on 429/5xx.
- Dates normalized to **YYYYMMDD**; numeric fields validated.
- IDs must be unique per `(source, id)`.
